# 输出权重路径output_dir，运行卡，port，dataset_path，$2
set -x
base_dir=/home/chenyushuo/MPO-albert_cuda
check_point_dir=/mnt/chenyushuo/checkpoint/academic-budget-bert/
data_dir_base=/mnt/liupeiyu/nlp_data/GLUE
    # --do_train \

#   --lr_scheduler_type polynomial \
function run_finetune() {
  export CUDA_VISIBLE_DEVICES=$1
  COMMON_ARGS="--model_name_or_path $4 \
  --task_name $3 \
  --max_seq_length 128 \
  --output_dir $check_point_dir/$2 \
  --overwrite_output_dir \
  --do_train \
  --do_eval \
  --evaluation_strategy steps \
  --per_device_train_batch_size 8 \
  --gradient_accumulation_steps 4 \
  --per_device_eval_batch_size 32 \
  --learning_rate 5e-5 \
  --weight_decay 0.01 \
  --eval_steps 50 \
  --evaluation_strategy steps \
  --max_grad_norm 1.0 \
  --num_train_epochs 5 \
  --load_best_model_at_end \
  --mpo_config=${base_dir}/mpo_shape_config.json \
  --runname=$2 \
  --warmup_steps 50 $5"
#   nohup python $base_dir/run_glue_mpoalbert.py --log_file_path=$2_$(date "+%Y%m%d-%H%M%S") \
#       ${COMMON_ARGS} > $base_dir/log_glue/$2_$(date "+%Y%m%d-%H%M%S").log 2>&1 &
#   python $base_dir/run_glue_mpoalbert.py --log_file_path=$2_$(date "+%Y%m%d-%H%M%S") ${COMMON_ARGS}
  ipdb3 $base_dir/run_glue_mpoalbert.py --log_file_path=$2_$(date "+%Y%m%d-%H%M%S") ${COMMON_ARGS}
}
# ############################################### 12 layer
# run_finetune 0 rte_mpo_albert_10k rte /mnt/liupeiyu/checkpoint/academic-budget-bert/run_mpo_12_albertmini_initCT_v2_lr00001_4/pretraining_experiment-/epoch1000000_step10324 --per_device_train_batch_size=32\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --learning_rate=5e-6\ --mpo_lr=5e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit 
# run_finetune [gpu] cola_mpo_albert_10k cola $check_point_dir/run_mpo_12_albert_initCT_3/epoch1000000_step10293 --per_device_train_batch_size=32\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=80\ --learning_rate=2e-6\ --mpo_lr=6e-6\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
# run_finetune [gpu] mrpc_mpo_albert_10k mrpc $check_point_dir/run_mpo_12_albert_initCT_3/epoch1000000_step10293 --per_device_train_batch_size=32\ --share_layer=0,12\ --num_hidden_groups=12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=20\ --learning_rate=5e-6\ --save_total_limit=1\ --metric_for_best_model="f1"\ --glue_model_type=albert-frominit
# run_finetune [gpu] stsb_mpo_albert_10k stsb $check_point_dir/run_mpo_12_albert_initCT_3/epoch1000000_step10293 --per_device_train_batch_size=16\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --max_steps=3598\ --warmup_steps=214\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --save_total_limit=1\ --metric_for_best_model="spearmanr"\ --glue_model_type=albert-frominit
# run_finetune [gpu] sst_mpo_albert_10k sst2 $check_point_dir/run_mpo_12_albert_initCT_3/epoch1000000_step10293 --per_device_train_batch_size=32\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] qqp_mpo_albert_10k qqp $check_point_dir/run_mpo_12_albert_initCT_3/epoch1000000_step10293 --per_device_train_batch_size=128\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --max_steps=14000\ --warmup_steps=1000\ --learning_rate=5e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] mnli_mpo_albert_10k mnli $check_point_dir/run_mpo_12_albert_initCT_3/epoch1000000_step10293 --per_device_train_batch_size=32\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] qnli_mpo_albert_10k qnli $check_point_dir/run_mpo_12_albert_initCT_3/epoch1000000_step10293 --per_device_train_batch_size=128\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=500\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] qnli_mpo_albert_10k qnli $check_point_dir/run_mpo_12_albert_initCT_3/epoch1000000_step10293 --per_device_train_batch_size=128\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=500\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit

# run_finetune [gpu] mrpc_mpo_albert_continuepre mrpc $check_point_dir/run_mpo_12_albertmini_albertinit_orialbert_3/global_step10269 --num_hidden_groups=1\ --mpo_layers=nompo\ --per_device_train_batch_size=32\ --save_total_limit=1\ --metric_for_best_model="f1"\ --glue_model_type=albert-frominit

# #### 24
# run_finetune [gpu] rte_mpo_24CT_10k rte /mnt/liupeiyu/checkpoint/academic_bert_224/run_mpo_24_albertmini_initCT_4/global_step10819 --per_device_train_batch_size=32\ --share_layer=0,24\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=24\ --learning_rate=5e-6\ --mpo_lr=5e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] rte_mpo_24CT_10k_2 rte /mnt/liupeiyu/checkpoint/academic_bert_224/run_mpo_24_albertmini_initCT_4/global_step10819 --per_device_train_batch_size=32\ --share_layer=0,24\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=24\ --warmup_steps=0\ --weight_decay=0.0\ --learning_rate=2e-6\ --mpo_lr=2e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] cola_mpo_24CT_10k cola /mnt/liupeiyu/checkpoint/academic_bert_224/run_mpo_24_albertmini_initCT_4/global_step10819 --per_device_train_batch_size=32\ --share_layer=0,24\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=24\ --warmup_steps=80\ --learning_rate=2e-6\ --mpo_lr=6e-6\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
# run_finetune [gpu] mrpc_mpo_24CT_10k mrpc /mnt/liupeiyu/checkpoint/academic_bert_224/run_mpo_24_albertmini_initCT_4/global_step10819 --per_device_train_batch_size=32\ --share_layer=0,24\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=24\ --warmup_steps=20\ --learning_rate=5e-6\ --save_total_limit=1\ --metric_for_best_model="f1"\ --glue_model_type=albert-frominit
# run_finetune [gpu] stsb_mpo_24CT_10k stsb /mnt/liupeiyu/checkpoint/academic_bert_224/run_mpo_24_albertmini_initCT_4/global_step10819 --per_device_train_batch_size=16\ --share_layer=0,24\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=24\ --max_steps=3598\ --warmup_steps=214\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --save_total_limit=1\ --metric_for_best_model="spearmanr"\ --glue_model_type=albert-frominit
# run_finetune [gpu] qqp_mpo_24CT_10k qqp /mnt/liupeiyu/checkpoint/academic_bert_224/run_mpo_24_albertmini_initCT_4/global_step10819 --per_device_train_batch_size=32\ --share_layer=0,24\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=24\ --warmup_steps=1000\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\  --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] sst2_mpo_24CT_10k sst2 /mnt/liupeiyu/checkpoint/academic_bert_224/run_mpo_24_albertmini_initCT_4/global_step10819 --per_device_train_batch_size=32\ --share_layer=0,24\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=24\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] mnli_mpo_24CT_10k mnli /mnt/liupeiyu/checkpoint/academic_bert_224/run_mpo_24_albertmini_initCT_4/global_step10819 --per_device_train_batch_size=32\ --share_layer=0,24\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=24\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] qnli_mpo_24CT_10k qnli /mnt/liupeiyu/checkpoint/academic_bert_224/run_mpo_24_albertmini_initCT_4/global_step10819 --per_device_train_batch_size=32\ --share_layer=0,24\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=24\ --warmup_steps=500\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit

# run_finetune [gpu] mrpc_mpo_24CT_10k_2 mrpc /mnt/liupeiyu/checkpoint/academic_bert_224/run_mpo_24_albertmini_initCT_4/global_step10819 --per_device_train_batch_size=32\ --share_layer=0,24\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=24\ --warmup_steps=0\ --learning_rate=1e-5\ --save_total_limit=1\ --metric_for_best_model="f1"\ --glue_model_type=albert-frominit
# run_finetune [gpu] mrpc_mpo_24CT_10k_3 mrpc /mnt/liupeiyu/checkpoint/academic_bert_224/run_mpo_24_albertmini_initCT_4/global_step10819 --per_device_train_batch_size=32\ --share_layer=0,24\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=24\ --warmup_steps=0\ --weight_decay=0.0\ --learning_rate=2e-6\ --save_total_limit=1\ --metric_for_best_model="f1"\ --glue_model_type=albert-frominit
# run_finetune [gpu] cola_mpo_24CT_10k_2 cola /mnt/liupeiyu/checkpoint/academic_bert_224/run_mpo_24_albertmini_initCT_4/global_step10819 --per_device_train_batch_size=32\ --share_layer=0,24\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=24\ --warmup_steps=0\ --weight_decay=0.0\ --learning_rate=2e-6\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
# run_finetune [gpu] rte_mpo_24CT_10k_3 rte /mnt/liupeiyu/checkpoint/academic_bert_224/run_mpo_24_albertmini_initCT_4/global_step10819 --per_device_train_batch_size=32\ --share_layer=0,24\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=24\ --warmup_steps=20\ --learning_rate=1.35e-5\ --weight_decay=0.0467984\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] rte_mpo_24CT_10k_4 rte /mnt/liupeiyu/checkpoint/academic_bert_224/run_mpo_24_albertmini_initCT_4/global_step10819 --per_device_train_batch_size=32\ --share_layer=0,24\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=24\ --warmup_steps=0\ --learning_rate=1.35e-5\ --weight_decay=0.0467984\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit

# #### 24_ori
# run_finetune [gpu] rte_ori_24CT_10k_2 rte /mnt/liupeiyu/checkpoint/albert-large-v2 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=0\ --weight_decay=0.0\ --learning_rate=2e-6\ --mpo_lr=2e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] cola_ori_24CT_10k2 cola /mnt/liupeiyu/checkpoint/albert-large-v2 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=80\ --learning_rate=2e-6\ --mpo_lr=6e-6\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
# run_finetune [gpu] mrpc_ori_24CT_10k2 mrpc /mnt/liupeiyu/checkpoint/albert-large-v2 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=20\ --learning_rate=5e-6\ --save_total_limit=1\ --metric_for_best_model="f1"\ --glue_model_type=albert-frominit
# run_finetune [gpu] stsb_ori_24CT_10k2 stsb /mnt/liupeiyu/checkpoint/albert-large-v2 --per_device_train_batch_size=16\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --max_steps=3598\ --warmup_steps=214\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --save_total_limit=1\ --metric_for_best_model="spearmanr"\ --glue_model_type=albert-frominit
# run_finetune [gpu] qqp_ori_24CT_10k2 qqp /mnt/liupeiyu/checkpoint/albert-large-v2 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=1000\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\  --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] sst2_ori_24CT_10k2 sst2 /mnt/liupeiyu/checkpoint/albert-large-v2 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] mnli_ori_24CT_10k2 mnli /mnt/liupeiyu/checkpoint/albert-large-v2 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] qnli_ori_24CT_10k2 qnli /mnt/liupeiyu/checkpoint/albert-large-v2 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=500\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit

# run_finetune [gpu] rte_ori_continue_20k rte /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step20378 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=0\ --weight_decay=0.0\ --learning_rate=2e-6\ --mpo_lr=2e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] cola_ori_continue_20k cola /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step20378 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=80\ --learning_rate=2e-6\ --mpo_lr=6e-6\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
# run_finetune [gpu] mrpc_ori_continue_20k mrpc /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step20378 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=20\ --learning_rate=5e-6\ --save_total_limit=1\ --metric_for_best_model="f1"\ --glue_model_type=albert-frominit
# run_finetune [gpu] stsb_ori_continue_20k stsb /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step20378 --per_device_train_batch_size=16\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --max_steps=3598\ --warmup_steps=214\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --save_total_limit=1\ --metric_for_best_model="spearmanr"\ --glue_model_type=albert-frominit
# run_finetune [gpu] qqp_ori_continue_20k qqp /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step20378 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=1000\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\  --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] sst2_ori_continue_20k sst2 /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step20378 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] mnli_ori_continue_20k mnli /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step20378 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] qnli_ori_continue_20k qnli /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step20378 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=500\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] cola_ori_continue_20k2 cola /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step20378 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=0\ --weight_decay=0.0\ --learning_rate=2e-6\ --mpo_lr=6e-6\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit

# run_finetune [gpu] rte_ori_continue_30k rte /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=0\ --weight_decay=0.0\ --learning_rate=2e-6\ --mpo_lr=2e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] cola_ori_continue_30k cola /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=80\ --learning_rate=2e-6\ --mpo_lr=6e-6\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
# run_finetune [gpu] mrpc_ori_continue_30k mrpc /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=20\ --learning_rate=5e-6\ --save_total_limit=1\ --metric_for_best_model="f1"\ --glue_model_type=albert-frominit
# run_finetune [gpu] stsb_ori_continue_30k stsb /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --per_device_train_batch_size=16\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --max_steps=3598\ --warmup_steps=214\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --save_total_limit=1\ --metric_for_best_model="spearmanr"\ --glue_model_type=albert-frominit
# run_finetune [gpu] qqp_ori_continue_30k qqp /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=1000\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\  --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] sst2_ori_continue_30k sst2 /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] mnli_ori_continue_30k mnli /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] qnli_ori_continue_30k qnli /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --num_hidden_groups=1\ --warmup_steps=500\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit

# run_finetune [gpu] rte_mpo_albert_continuepre_30k_2 rte /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --do_train\ --num_hidden_groups=1\ --mpo_layers=nompo\ --per_device_train_batch_size=32\ --learning_rate=5e-5\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] rte_mpo_albert_continuepre_20k_2 rte /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step20378 --do_train\ --num_hidden_groups=1\ --mpo_layers=nompo\ --per_device_train_batch_size=32\ --learning_rate=5e-5\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# # run_finetune 2 cola_mpo_albert_continuepre cola /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --do_train\ --num_hidden_groups=1\ --warmup_steps=80\ --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --learning_rate=2e-6\ --eval_steps=1000\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
# # run_finetune 5 cola_mpo_albert_continuepre_2 cola /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --do_train\ --num_hidden_groups=1\ --warmup_steps=80\ --per_device_train_batch_size=16\ --mpo_layers=nompo\ --load_layer=noload\ --learning_rate=2e-6\ --eval_steps=1000\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
# run_finetune [gpu] cola_ori_continue_30k_2 cola /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --do_train\ --num_hidden_groups=1\ --warmup_steps=80\ --per_device_train_batch_size=16\ --mpo_layers=nompo\ --load_layer=noload\ --learning_rate=2e-5\ --eval_steps=1000\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
# run_finetune [gpu] cola_ori_continue_20k_2 cola /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step20378 --do_train\ --num_hidden_groups=1\ --warmup_steps=80\ --per_device_train_batch_size=16\ --mpo_layers=nompo\ --load_layer=noload\ --learning_rate=2e-5\ --eval_steps=1000\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
# run_finetune [gpu] cola_ori_continue_30k_3 cola /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --do_train\ --num_hidden_groups=1\ --warmup_steps=0\ --weight_decay=0.0\ --per_device_train_batch_size=16\ --mpo_layers=nompo\ --load_layer=noload\ --learning_rate=2e-5\ --eval_steps=1000\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
# run_finetune [gpu] cola_ori_continue_20k_3 cola /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step20378 --do_train\ --num_hidden_groups=1\ --warmup_steps=0\ --weight_decay=0.0\ --per_device_train_batch_size=16\ --mpo_layers=nompo\ --load_layer=noload\ --learning_rate=2e-5\ --eval_steps=1000\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
# # run_finetune 2 rte_mpo_albert_continuepre_2 rte /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --do_train\ --num_hidden_groups=1\ --mpo_layers=nompo\ --per_device_train_batch_size=32\ --learning_rate=5e-5\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] sst_ori_continue_30k_2 sst2 /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --do_train\ --num_hidden_groups=1\ --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# # run_finetune 2 mnli_mpo_albert_continuepre mnli /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --do_train\ --num_hidden_groups=1\ --per_device_train_batch_size=32\ --mpo_layers=nompo\ --load_layer=noload\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# # run_finetune 4 qqp_mpo_albert_continuepre qqp /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --do_train\ --num_hidden_groups=1\ --per_device_train_batch_size=128\ --share_layer=0,12\ --mpo_layers=nompo\ --load_layer=noload\ --max_steps=14000\ --warmup_steps=1000\ --learning_rate=5e-6\ --eval_steps=1000\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# # run_finetune 5 qnli_mpo_albert_continuepre qnli /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --do_train\ --num_hidden_groups=1\ --per_device_train_batch_size=128\ --mpo_layers=nompo\ --load_layer=noload\ --warmup_steps=500\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# # run_finetune 2 mrpc_mpo_albert_continuepre mrpc /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --do_train\ --per_device_train_batch_size=32\ --num_hidden_groups=1\ --mpo_layers=nompo\ --load_layer=noload\ --warmup_steps=20\ --learning_rate=5e-6\ --save_total_limit=1\ --metric_for_best_model="f1"\ --glue_model_type=albert-frominit
# # run_finetune 2 stsb_mpo_albert_continuepre stsb /mnt/liupeiyu/checkpoint/academic_bert_154/run_mpo_12_albertmini_albertinit_orialbert_4/global_step30616 --do_train\ --per_device_train_batch_size=16\ --num_hidden_groups=1\ --mpo_layers=nompo\ --load_layer=noload\ --max_steps=3598\ --warmup_steps=214\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --save_total_limit=1\ --metric_for_best_model="spearmanr"\ --glue_model_type=albert-frominit

# ### 48layer 7k
# run_finetune [gpu] rte_mpo_48CT_7k rte $check_point_dir/run_mpo_48_albert24_initCT_2/global_step7048 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --learning_rate=5e-7\ --mpo_lr=5e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] rte_mpo_48CT_7k_2 rte $check_point_dir/run_mpo_48_albert24_initCT_2/global_step7048 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --learning_rate=5e-6\ --mpo_lr=5e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] rte_mpo_48CT_7k_3 rte $check_point_dir/run_mpo_48_albert24_initCT_2/global_step7048 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --learning_rate=5e-8\ --mpo_lr=5e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] sst2_mpo_48CT_7k sst2 $check_point_dir/run_mpo_48_albert24_initCT_2/global_step7048 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] sst2_mpo_48CT_7k_2 sst2 $check_point_dir/run_mpo_48_albert24_initCT_2/global_step7048 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=500\ --learning_rate=2e-7\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] mrpc_mpo_48CT_7k mrpc $check_point_dir/run_mpo_48_albert24_initCT_2/global_step7048 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=20\ --learning_rate=5e-6\ --save_total_limit=1\ --metric_for_best_model="f1"\ --glue_model_type=albert-frominit
# run_finetune [gpu] cola_mpo_48CT_7k cola $check_point_dir/run_mpo_48_albert24_initCT_2/global_step7048 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=80\ --learning_rate=2e-6\ --mpo_lr=6e-6\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
# run_finetune [gpu] qqp_mpo_48CT_7k qqp $check_point_dir/run_mpo_48_albert24_initCT_2/global_step7048 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --max_steps=14000\ --warmup_steps=1000\ --learning_rate=5e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] stsb_mpo_48CT_7k stsb $check_point_dir/run_mpo_48_albert24_initCT_2/global_step7048 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --max_steps=3598\ --warmup_steps=214\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --save_total_limit=1\ --metric_for_best_model="spearmanr"\ --glue_model_type=albert-frominit
# run_finetune [gpu] mnli_mpo_48CT_7k mnli $check_point_dir/run_mpo_48_albert24_initCT_2/global_step7048 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune [gpu] qnli_mpo_48CT_7k qnli $check_point_dir/run_mpo_48_albert24_initCT_2/global_step7048 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=500\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit

# run_finetune [gpu] rte_mpo_48CT_7k_4 rte $check_point_dir/run_mpo_48_albert24_initCT_2/global_step7048 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=0\ --weight_decay=0.0\ --learning_rate=2e-6\ --mpo_lr=2e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit\ --gradient_accumulation_steps=2
# run_finetune [gpu] mrpc_mpo_48CT_7k_2 mrpc $check_point_dir/run_mpo_48_albert24_initCT_2/global_step7048 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=0\ --weight_decay=0.0\ --learning_rate=2e-6\ --save_total_limit=1\ --metric_for_best_model="f1"\ --glue_model_type=albert-frominit\ --gradient_accumulation_steps=2

# ### 48layer 9k
# run_finetune [gpu] rte_mpo_48CT_9k rte $check_point_dir/run_mpo_48_albert24_initCT_2/global_step8976 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=0\ --weight_decay=0.0\ --learning_rate=2e-6\ --mpo_lr=2e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit\ --gradient_accumulation_steps=2
# run_finetune [gpu] rte_mpo_48CT_9k_2 rte $check_point_dir/run_mpo_48_albert24_initCT_2/global_step8976 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=0\ --weight_decay=0.0\ --learning_rate=2e-7\ --mpo_lr=2e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit\ --gradient_accumulation_steps=2
# run_finetune [gpu] rte_mpo_48CT_9k_3 rte $check_point_dir/run_mpo_48_albert24_initCT_2/global_step8976 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=0\ --weight_decay=0.0\ --learning_rate=2e-6\ --mpo_lr=2e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit\ --gradient_accumulation_steps=8
# run_finetune [gpu] rte_mpo_48CT_9k_4 rte $check_point_dir/run_mpo_48_albert24_initCT_2/global_step8976 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=0\ --weight_decay=0.0\ --learning_rate=2e-7\ --mpo_lr=2e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit\ --gradient_accumulation_steps=8
# run_finetune [gpu] rte_mpo_48CT_9k_5 rte $check_point_dir/run_mpo_48_albert24_initCT_2/global_step8976 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=0\ --weight_decay=0.0\ --learning_rate=5e-7\ --mpo_lr=2e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit\ --gradient_accumulation_steps=8
# run_finetune [gpu] stsb_mpo_48CT_9k stsb $check_point_dir/run_mpo_48_albert24_initCT_2/global_step8976 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --max_steps=3598\ --warmup_steps=214\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --save_total_limit=1\ --metric_for_best_model="spearmanr"\ --glue_model_type=albert-frominit
# run_finetune [gpu] sst2_mpo_48CT_9k sst2 $check_point_dir/run_mpo_48_albert24_initCT_2/global_step8976 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit\ --gradient_accumulation_steps=8

### 48layer 10k
# run_finetune [gpu] rte_mpo_48CT_10k rte $check_point_dir/run_mpo_48_albert24_initCT_2/global_step10272 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=0\ --weight_decay=0.287442\ --learning_rate=1.75435e-06\ --mpo_lr=1.75435e-06\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit\ --gradient_accumulation_steps=4
# run_finetune [gpu] rte_mpo_48CT_9k rte $check_point_dir/run_mpo_48_albert24_initCT_2/global_step8976 --do_train\ --per_device_train_batch_size=16\ --share_layer=0,48\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --num_hidden_groups=48\ --warmup_steps=0\ --weight_decay=0.287442\ --learning_rate=1.75435e-06\ --mpo_lr=1.75435e-06\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit\ --gradient_accumulation_steps=4


################### new
run_finetune 0 rte_mpo_albert_10k_10253 rte $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/global_step10253 --per_device_train_batch_size=32\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --learning_rate=5e-6\ --mpo_lr=5e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit 
run_finetune 1 cola_mpo_albert_10k_10253 cola $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/global_step10253 --per_device_train_batch_size=32\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=80\ --learning_rate=2e-6\ --mpo_lr=6e-6\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
run_finetune 2 mrpc_mpo_albert_10k_10253 mrpc $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/global_step10253 --per_device_train_batch_size=32\ --share_layer=0,12\ --num_hidden_groups=12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=20\ --learning_rate=5e-6\ --save_total_limit=1\ --metric_for_best_model="f1"\ --glue_model_type=albert-frominit
run_finetune 0 stsb_mpo_albert_10k_10253 stsb $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/global_step10253 --per_device_train_batch_size=16\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --max_steps=3598\ --warmup_steps=214\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --save_total_limit=1\ --metric_for_best_model="spearmanr"\ --glue_model_type=albert-frominit
run_finetune 1 sst_mpo_albert_10k_10253 sst2 $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/global_step10253 --per_device_train_batch_size=32\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
run_finetune 2 qqp_mpo_albert_10k_10253 qqp $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/global_step10253 --per_device_train_batch_size=128\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --max_steps=14000\ --warmup_steps=1000\ --learning_rate=5e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
run_finetune 0 mnli_mpo_albert_10k_10253 mnli $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/global_step10253 --per_device_train_batch_size=32\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
run_finetune 1 qnli_mpo_albert_10k_10253 qnli $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/global_step10253 --per_device_train_batch_size=128\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=500\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit

# wait

# run_finetune 0 rte_mpo_albert_10k_20440 rte $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/epoch1000000_step20440 --per_device_train_batch_size=32\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --learning_rate=5e-6\ --mpo_lr=5e-6\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit 
# run_finetune 1 cola_mpo_albert_10k_20440 cola $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/epoch1000000_step20440 --per_device_train_batch_size=32\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=80\ --learning_rate=2e-6\ --mpo_lr=6e-6\ --save_total_limit=1\ --metric_for_best_model="matthews_correlation"\ --glue_model_type=albert-frominit
# run_finetune 2 mrpc_mpo_albert_10k_20440 mrpc $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/epoch1000000_step20440 --per_device_train_batch_size=32\ --share_layer=0,12\ --num_hidden_groups=12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=20\ --learning_rate=5e-6\ --save_total_limit=1\ --metric_for_best_model="f1"\ --glue_model_type=albert-frominit
# run_finetune 3 stsb_mpo_albert_10k_20440 stsb $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/epoch1000000_step20440 --per_device_train_batch_size=16\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --max_steps=3598\ --warmup_steps=214\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --save_total_limit=1\ --metric_for_best_model="spearmanr"\ --glue_model_type=albert-frominit
# run_finetune 4 sst_mpo_albert_10k_20440 sst2 $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/epoch1000000_step20440 --per_device_train_batch_size=32\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune 5 qqp_mpo_albert_10k_20440 qqp $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/epoch1000000_step20440 --per_device_train_batch_size=128\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --max_steps=14000\ --warmup_steps=1000\ --learning_rate=5e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune 6 mnli_mpo_albert_10k_20440 mnli $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/epoch1000000_step20440 --per_device_train_batch_size=32\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=500\ --learning_rate=2e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
# run_finetune 7 qnli_mpo_albert_10k_20440 qnli $check_point_dir/run_mpo_12_test_fp16/pretraining_experiment-/epoch1000000_step20440 --per_device_train_batch_size=128\ --share_layer=0,12\ --mpo_layers=FFN_1,FFN_2,attention\ --load_layer=FFN_1,FFN_2,attention\ --warmup_steps=500\ --learning_rate=2e-6\ --mpo_lr=2.8e-6\ --eval_steps=1000\ --num_train_epochs=3\ --save_total_limit=1\ --metric_for_best_model="accuracy"\ --glue_model_type=albert-frominit
